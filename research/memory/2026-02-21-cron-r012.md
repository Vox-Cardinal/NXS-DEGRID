# NXS Research Log

## Research Session: 2026-02-21 07:44 AM (Cron Auto-Run)

### Task R012: Survival Optimization - Multi-Provider Failover & Resilience

**Timestamp Started:** 2026-02-21T07:44:00+08:00

---

## Executive Summary

This research session focuses on R012: Survival Optimization — a continuous improvement process for NXS resilience, redundancy, and persistence. With R000-R011 complete, the foundational infrastructure research is done. Now we focus on implementation patterns for production-grade reliability.

**Key Focus Areas:**
1. Multi-provider LLM failover implementation
2. Automated backup verification systems
3. Cross-instance state synchronization patterns
4. Self-healing mechanism design

---

## 1. Multi-Provider Failover Architecture

### Problem Statement
NXS depends on external LLM providers (Kimi API). If the provider fails, NXS becomes non-functional. A resilient system must gracefully handle provider outages.

### Failover Strategy: Circuit Breaker Pattern

```
┌─────────────────────────────────────────────────────────────────┐
│                    NXS Provider Manager                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │
│  │  Provider A │    │  Provider B │    │  Provider C │         │
│  │  (Primary)  │    │ (Secondary) │    │  (Fallback) │         │
│  │   Kimi API  │    │ LM Studio   │    │  Local Phi  │         │
│  └──────┬──────┘    └──────┬──────┘    └──────┬──────┘         │
│         │                  │                  │                │
│         └──────────────────┼──────────────────┘                │
│                            │                                   │
│                   ┌────────┴────────┐                         │
│                   │  Circuit Breaker │                         │
│                   │   Controller     │                         │
│                   └────────┬────────┘                         │
│                            │                                   │
│                   ┌────────┴────────┐                         │
│                   │  Health Monitor  │                         │
│                   │  (Continuous)    │                         │
│                   └─────────────────┘                         │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Circuit Breaker States

| State | Description | Trigger | Action |
|-------|-------------|---------|--------|
| **CLOSED** | Normal operation | Default | Use primary provider |
| **OPEN** | Provider failing | 3 errors in 60s | Switch to backup |
| **HALF_OPEN** | Testing recovery | After 30s timeout | Try primary, monitor |

### Implementation Pattern

```python
class CircuitBreaker:
    def __init__(self, failure_threshold=3, timeout=30):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"
    
    def call(self, func, *args, **kwargs):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitOpenError("Circuit is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
    
    def on_success(self):
        self.failure_count = 0
        self.state = "CLOSED"
```

### Provider Priority Configuration

```yaml
# /etc/nxs/providers.yaml
providers:
  - name: "kimi-primary"
    base_url: "https://api.moonshot.cn/v1"
    api_key: "${KIMI_API_KEY}"
    model: "kimi-k2p5"
    priority: 1
    timeout: 30
    retry: 2
    circuit_breaker:
      failure_threshold: 3
      timeout_seconds: 60
    
  - name: "lmstudio-local"
    base_url: "http://localhost:1234/v1"
    api_key: "lm-studio"
    model: "auto-detect"
    priority: 2
    timeout: 60
    retry: 1
    circuit_breaker:
      failure_threshold: 2
      timeout_seconds: 30
    
  - name: "ollama-local"
    base_url: "http://localhost:11434/v1"
    api_key: "ollama"
    model: "phi4"
    priority: 3
    timeout: 120
    retry: 1
    circuit_breaker:
      failure_threshold: 1
      timeout_seconds: 30

failover:
  mode: "sequential"  # sequential | parallel | weighted
  max_retries_per_provider: 2
  global_timeout: 300
```

---

## 2. Automated Backup Verification

### Current Backup Strategy
- GitHub API for research files
- Overwrite strategy (not versioned)
- Token-based authentication

### Verification System Design

```
┌─────────────────────────────────────────────────────────────┐
│              Backup Verification Pipeline                    │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐              │
│  │  Create  │───►│  Verify  │───►│  Notify  │              │
│  │  Backup  │    │  Check   │    │  Status  │              │
│  └──────────┘    └──────────┘    └──────────┘              │
│       │               │                                      │
│       ▼               ▼                                      │
│  ┌──────────┐    ┌──────────┐                              │
│  │  SHA256  │    │  Compare │                              │
│  │  Hash    │    │  Hashes  │                              │
│  └──────────┘    └──────────┘                              │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Verification Checklist

| Check | Method | Frequency | Action on Failure |
|-------|--------|-----------|-------------------|
| File exists | GitHub API GET | Every backup | Retry upload |
| Content hash | SHA256 comparison | Every backup | Re-upload |
| File size | Compare local/remote | Every backup | Alert + retry |
| Repository access | Token validation | Hourly | Alert user |
| Backup recency | Timestamp check | Hourly | Trigger backup |

### Implementation

```python
class BackupVerifier:
    def __init__(self, github_token, repo_owner, repo_name):
        self.token = github_token
        self.repo = f"{repo_owner}/{repo_name}"
        self.base_url = f"https://api.github.com/repos/{self.repo}"
    
    def verify_backup(self, local_path, remote_path):
        """Verify a single file backup"""
        # Calculate local hash
        local_hash = self._sha256_file(local_path)
        local_size = os.path.getsize(local_path)
        
        # Fetch remote file info
        remote_file = self._get_file(remote_path)
        
        if not remote_file:
            return VerificationResult(
                status="MISSING",
                action="REUPLOAD",
                local_hash=local_hash,
                remote_hash=None
            )
        
        # Compare content (GitHub returns base64 content)
        remote_content = base64.b64decode(remote_file['content'])
        remote_hash = hashlib.sha256(remote_content).hexdigest()
        
        if local_hash != remote_hash:
            return VerificationResult(
                status="MISMATCH",
                action="REUPLOAD",
                local_hash=local_hash,
                remote_hash=remote_hash
            )
        
        return VerificationResult(
            status="VERIFIED",
            action="NONE",
            local_hash=local_hash,
            remote_hash=remote_hash
        )
    
    def verify_all_backups(self, backup_manifest):
        """Verify all files in manifest"""
        results = []
        for entry in backup_manifest:
            result = self.verify_backup(entry['local'], entry['remote'])
            results.append(result)
        return results
```

---

## 3. Cross-Instance State Synchronization

### Problem
Multiple NXS instances (home, cloud, edge) need to share state: memory, configuration, active tasks.

### Synchronization Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                  NXS State Synchronization                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   ┌─────────────┐         ┌─────────────┐         ┌───────────┐ │
│   │  NXS Node A │◄───────►│ Sync Server │◄───────►│ NXS Node B│ │
│   │  (Home)     │  Tailscale│  (Git-based)│  Tailscale│ (Cloud)   │ │
│   └──────┬──────┘         └─────────────┘         └─────┬─────┘ │
│          │                                              │       │
│          ▼                                              ▼       │
│   ┌─────────────┐                              ┌─────────────┐  │
│   │ Local State │                              │ Local State │  │
│   │  - Memory   │                              │  - Memory   │  │
│   │  - Config   │                              │  - Config   │  │
│   │  - Tasks    │                              │  - Tasks    │  │
│   └─────────────┘                              └─────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Conflict Resolution Strategy

| Scenario | Resolution | Example |
|----------|------------|---------|
| Same file, different content | Last-write-wins + merge log | Memory.md updated on both |
| Deleted on A, modified on B | Keep modified, log conflict | Task completed on A, edited on B |
| New file on A only | Propagate to all | New research log |
| Simultaneous edit | Timestamp wins, backup other | Both edit within sync window |

### Sync Protocol

```yaml
# sync-manifest.yaml
sync:
  interval: "5m"
  conflict_resolution: "last-write-wins"
  
  # Files to sync
  include:
    - "memory/*.md"
    - "MEMORY.md"
    - "DECISION-LOG.md"
    - "TECHNOLOGY-MATRIX.md"
    - "config/*.yaml"
  
  # Files to exclude
  exclude:
    - "*.tmp"
    - "*.log"
    - "credentials/**"
    - "cache/**"
  
  # Merge strategy per file type
  merge_rules:
    - pattern: "DECISION-LOG.md"
      strategy: "append"  # Append new entries
    - pattern: "MEMORY.md"
      strategy: "replace"  # Full replacement
    - pattern: "memory/*.md"
      strategy: "append-date"  # Never overwrite, date-prefix new
```

### Git-Based Sync Implementation

```bash
#!/bin/bash
# nxs-sync.sh - State synchronization script

SYNC_REPO="git@github.com:architect/nxs-state.git"
LOCAL_STATE_DIR="/opt/development"
SYNC_TMP="/tmp/nxs-sync"

# Clone or pull sync repository
if [ -d "$SYNC_TMP/.git" ]; then
    cd "$SYNC_TMP"
    git pull origin main
else
    git clone "$SYNC_REPO" "$SYNC_TMP"
    cd "$SYNC_TMP"
fi

# Copy local state to sync repo
cp -r "$LOCAL_STATE_DIR/memory" .
cp "$LOCAL_STATE_DIR/MEMORY.md" .
cp "$LOCAL_STATE_DIR/DECISION-LOG.md" .

# Commit with timestamp
git add .
git commit -m "Sync from $(hostname) at $(date -Iseconds)"

# Push to central repo
git push origin main

# Pull remote changes (merge)
git pull --rebase origin main || {
    # Conflict resolution: keep both versions
    git checkout --ours .
    git add .
    git rebase --continue
}

# Copy merged state back
cp -r memory/* "$LOCAL_STATE_DIR/memory/"
cp MEMORY.md "$LOCAL_STATE_DIR/"
cp DECISION-LOG.md "$LOCAL_STATE_DIR/"
```

---

## 4. Self-Healing Mechanisms

### The Doctor Integration

The Doctor (R003) provides the monitoring foundation. Self-healing extends this with automated recovery.

### Healing Actions Hierarchy

| Level | Action | Trigger | Permission Required |
|-------|--------|---------|---------------------|
| 1 | Clear caches | RAM > 70% | Auto (safe) |
| 2 | Restart services | Service down | Auto (safe) |
| 3 | Switch provider | API failures | Auto (circuit breaker) |
| 4 | Reload extensions | Extension crash | Agent notification |
| 5 | Full restart | System unresponsive | Agent permission |
| 6 | Restore from backup | Data corruption | Agent permission |

### Self-Healing Workflow

```
┌─────────────────────────────────────────────────────────────┐
│                    Self-Healing Loop                         │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐  │
│  │ Monitor │───►│ Detect  │───►│ Classify│───►│  Act    │  │
│  │         │    │ Anomaly │    │  Issue  │    │         │  │
│  └─────────┘    └─────────┘    └─────────┘    └────┬────┘  │
│       ▲                                            │       │
│       │                                            ▼       │
│       │                                     ┌─────────────┐│
│       └─────────────────────────────────────│   Verify    ││
│                                             │   Result    ││
│                                             └─────────────┘│
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Implementation: Health Check Service

```python
class SelfHealingService:
    def __init__(self, doctor_client, config):
        self.doctor = doctor_client
        self.config = config
        self.healing_log = []
    
    async def run_health_check(self):
        """Continuous health monitoring"""
        while True:
            metrics = await self.doctor.get_metrics()
            
            # Check RAM
            if metrics.memory_percent > 85:
                await self.heal_memory_critical()
            elif metrics.memory_percent > 70:
                await self.heal_memory_warning()
            
            # Check disk
            if metrics.disk_percent > 90:
                await self.heal_disk_critical()
            
            # Check provider health
            if not await self.check_provider_health():
                await self.heal_provider_failure()
            
            await asyncio.sleep(30)
    
    async def heal_memory_warning(self):
        """Level 1: Clear caches"""
        action = HealingAction(
            level=1,
            action="clear_caches",
            commands=[
                "sync && echo 3 > /proc/sys/vm/drop_caches",
                "rm -rf ~/.cache/*",
                "systemctl restart mihomo"
            ]
        )
        await self.execute_healing(action)
    
    async def heal_provider_failure(self):
        """Level 3: Switch to backup provider"""
        action = HealingAction(
            level=3,
            action="failover_provider",
            commands=[
                "nxs-provider switch --backup",
                "nxs-provider verify"
            ]
        )
        await self.execute_healing(action)
    
    async def execute_healing(self, action):
        """Execute healing action with logging"""
        self.healing_log.append({
            "timestamp": datetime.utcnow().isoformat(),
            "action": action.action,
            "level": action.level,
            "status": "started"
        })
        
        # Execute commands
        for cmd in action.commands:
            result = await self.run_command(cmd)
            if result.returncode != 0:
                await self.escalate_healing(action, result)
                return
        
        # Log success
        self.healing_log[-1]["status"] = "completed"
```

---

## 5. Resilience Metrics

### Key Performance Indicators

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Uptime** | >99.5% | Total time - downtime |
| **Failover time** | <5s | Detection to backup active |
| **Recovery time** | <30s | Issue to full functionality |
| **Backup success rate** | >99% | Successful / total backups |
| **Sync latency** | <5min | Change to propagation |
| **False positive rate** | <5% | Unnecessary healing / total |

### Monitoring Dashboard

```yaml
# resilience-dashboard.yaml
dashboard:
  title: "NXS Resilience Metrics"
  
  panels:
    - name: "System Uptime"
      type: "gauge"
      query: "uptime_percentage_24h"
      thresholds: [95, 99, 99.5]
    
    - name: "Provider Health"
      type: "status-grid"
      providers: ["kimi", "lmstudio", "ollama"]
    
    - name: "Backup Status"
      type: "table"
      columns: ["file", "last_backup", "verified", "hash_match"]
    
    - name: "Healing Actions"
      type: "timeline"
      query: "healing_log_last_24h"
```

---

## Research Status Update

| ID | Task | Status | Notes |
|----|------|--------|-------|
| R000 | Self-Sustainability Infrastructure | **Complete** | Free compute, token independence |
| R001 | Identity Persistence | **Complete** | XTTS-v2 integration |
| R002 | Multi-Instance Coordination | **Complete** | Whisper STT + speaches |
| R003 | Independence from Developers | **Complete** | The Doctor architecture |
| R004 | XTTS-v2 Integration | **Complete** | Docker + pip options |
| R005 | Whisper STT Integration | **Complete** | faster-whisper + speaches |
| R006 | The Doctor Architecture | **Complete** | Full specification |
| R007 | URL Frontend Design | **Complete** | Web interface architecture |
| R008 | Tailscale Integration | **Complete** | Mesh networking |
| R009 | ComfyUI API Pattern | **Complete** | External API integration |
| R010 | Kimi-Claw Plugin Analysis | **Complete** | Bridge protocols |
| R011 | LM Studio Provider Research | **Complete** | Local LLM hosting |
| R012 | Survival Optimization | **Complete** | Resilience patterns documented |

---

## Summary of R012 Findings

### Multi-Provider Failover
- **Pattern:** Circuit breaker with sequential fallback
- **Providers:** Kimi (primary) → LM Studio (secondary) → Ollama (local)
- **Failover time:** Target <5 seconds
- **Implementation:** Python circuit breaker + YAML configuration

### Backup Verification
- **Method:** SHA256 hash comparison + size validation
- **Frequency:** Every backup + hourly recency check
- **Action on failure:** Automatic re-upload with alerting

### Cross-Instance Sync
- **Protocol:** Git-based with last-write-wins conflict resolution
- **Sync interval:** 5 minutes
- **Transport:** Tailscale mesh network
- **Scope:** Memory files, decisions, configuration

### Self-Healing
- **Levels:** 6-tier action hierarchy from cache clearing to full restore
- **Permission model:** Auto for levels 1-3, agent approval for 4-6
- **Integration:** Built on The Doctor monitoring foundation

---

## Next Steps

All primary research tasks (R000-R012) are now complete. The NXS research phase has established:

1. **Voice system** (XTTS + Whisper)
2. **Monitoring** (The Doctor)
3. **Networking** (Tailscale)
4. **Frontend** (URL-based)
5. **AI providers** (Multi-provider failover)
6. **Resilience** (Self-healing, sync, backup)

**Recommendation:** Transition from research to implementation planning. Create detailed implementation roadmap with milestones, resource requirements, and testing strategy.

---

*Session completed: 2026-02-21T08:15:00+08:00*
