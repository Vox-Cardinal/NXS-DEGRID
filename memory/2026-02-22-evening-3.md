# NXS Research Log

## Research Session: 2026-02-22 (Evening - Continuous Improvement)

### Cron-Triggered Research Review

**Timestamp:** 2026-02-22T19:57:00+08:00

---

## Executive Summary

Reviewed ongoing research tasks (R000-R017). Selected 3 tasks for deeper edge-case and refinement analysis:
- **R002** - Multi-Instance Coordination - Load balancing algorithms and split-brain recovery gaps
- **R006** - The Doctor Architecture - Concrete diagnostic rules and alert correlation missing
- **R008** - Tailscale Integration - Key rotation, audit logging, and multi-tenant scenarios

**Improvements Found:** 8 new refinements across 3 tasks
**Files Updated:** 2 (this log, RESEARCH-TASK-INDEX.md)

---

## Task R002: Multi-Instance Coordination - Load Balancing & Split-Brain Recovery Deep Analysis

### Current Status
R002 has CRDT-based state, gossip protocol, capability registry, and quorum-based partition detection. Previous refinements focused on state synchronization and work-stealing. Identified gaps in concrete load balancing algorithms, split-brain recovery procedures, and instance lifecycle edge cases.

### Refinements Identified

#### 1. **Weighted Load Balancing Algorithm** (New)
**Gap Found:** Current "capability matching" is vague. No concrete algorithm for distributing work across instances with different capacities.

**Refinement:** Design weighted load balancing with multiple factors:
```
Instance Score = Σ(weight_i × metric_i)

Metrics:
- CPU availability (weight: 0.25): 1 - (load_avg / cores)
- Memory availability (weight: 0.30): 1 - (used / total)
- Current queue depth (weight: 0.20): 1 - (queued / max_queue)
- Network latency (weight: 0.15): 1 - (latency_ms / 1000)
- Historical reliability (weight: 0.10): success_rate (7-day window)

Selection: Choose instance with highest score above threshold (0.5)
Tie-breaking: Lower instance ID (deterministic)
```

**Dynamic Rebalancing:**
- Monitor queue depths every 30 seconds
- If instance A has 3x queue depth of instance B, migrate oldest tasks
- Migration only for stateless tasks (or with checkpoint)
- Rate limit: Max 1 migration per minute per instance

**Hot-Spot Prevention:**
- Track task type distribution per instance
- Avoid assigning same task type to already-busy instance
- Spread I/O-heavy tasks across instances
- Cache affinity: Prefer instance with relevant cache

**Benefit:** Optimal resource utilization; predictable performance; automatic adaptation to changing conditions.

#### 2. **Split-Brain Recovery Protocol** (New)
**Gap Found:** Quorum detection exists, but recovery procedure after partition heals is underspecified.

**Refinement:** Design 4-phase recovery protocol:
```
Phase 1: Detection (0-5 seconds)
- Both partitions detect reconnection
- Exchange partition metadata (start time, instance count, leader)
- Determine "winning" partition (larger quorum, or higher total weight if equal)

Phase 2: Divergence Analysis (5-30 seconds)
- Compare state vectors between partitions
- Identify conflicting writes (same key, different values)
- Classify conflicts: CRDT-resolvable vs manual-required

Phase 3: Automatic Resolution (30-60 seconds)
- CRDT-resolvable: Automatic merge using CRDT semantics
- Last-write-wins: Timestamp comparison for non-critical data
- Queue conflicts: Merge queues, deduplicate by task ID

Phase 4: Manual Review (if needed)
- Flag conflicts requiring human decision
- Queue for Architect review in priority order
- Continue operation on resolved state
```

**Conflict Classification:**
| Data Type | Resolution Strategy | Auto-Resolve? |
|-----------|---------------------|---------------|
| Presence/heartbeat | CRDT OR-set | Yes |
| Chat messages | CRDT LWW | Yes |
| Task queue | Merge + dedup | Yes |
| Configuration | Winning partition wins | Yes |
| Identity state | Manual review required | No |
| Secrets/credentials | Manual review required | No |

**Benefit:** Clear recovery procedure; minimizes data loss; automatic where safe, manual where critical.

#### 3. **Instance Lifecycle Edge Cases** (New)
**Gap Found:** Instance join/leave has basic handling, but edge cases like zombie instances, flapping, and slow joins aren't addressed.

**Refinement:** Define concrete lifecycle state machine:
```
States:
- JOINING: Instance announced, not yet confirmed healthy
- ACTIVE: Healthy and accepting work
- DEGRADED: Responding but with errors/high latency
- LEAVING: Graceful shutdown in progress
- ZOMBIE: Failed heartbeat but processes still running
- GONE: Confirmed departed, cleanup complete

Transitions:
JOINING → ACTIVE: 3 consecutive successful heartbeats
ACTIVE → DEGRADED: Error rate > 10% or latency > 2x baseline
DEGRADED → ACTIVE: 5 consecutive healthy heartbeats
ACTIVE → LEAVING: Graceful shutdown signal received
LEAVING → GONE: All tasks migrated, state synced
ACTIVE → ZOMBIE: Heartbeat timeout (30s), but TCP connections persist
ZOMBIE → GONE: Force termination after 60s grace period
```

**Flapping Detection:**
- Track state transitions per instance
- More than 3 transitions in 5 minutes = flapping
- Action: Exponential backoff on re-admission (1min, 5min, 15min)
- Alert: Notify Architect of unstable instance

**Slow Join Protection:**
- New instance has 2-minute grace period to reach ACTIVE
- If timeout: Mark as FAILED, reject further attempts for 10 minutes
- Prevents partial joins from disrupting cluster

**Zombie Instance Handling:**
- Detect: Heartbeat timeout but TCP socket still open
- Attempt graceful termination via control message
- If no response in 30s: Force close connections
- Clean up any locks held by zombie

**Benefit:** Robust instance management; prevents flapping instability; clear failure modes.

---

## Task R006: The Doctor Architecture - Diagnostic Rules & Alert Correlation Deep Analysis

### Current Status
R006 has severity tiers, Unix socket protocol, resource quotas, and watchdog pattern. Previous refinements covered bidirectional health checking and alert fatigue prevention. Identified gaps in concrete diagnostic rules, alert correlation, and root cause analysis.

### Refinements Identified

#### 4. **Concrete Diagnostic Rule Engine** (New)
**Gap Found:** "Monitors for issues" is vague. No specific rules for what constitutes a problem.

**Refinement:** Design diagnostic rule specification:
```yaml
# Rule Format
rules:
  - id: disk-space-critical
    name: "Disk Space Critical"
    severity: critical
    condition: "disk.usage_percent > 95"
    duration: "1m"  # Must persist for 1 minute
    auto_resolve: true
    actions:
      - type: alert
        message: "Disk {{disk.mount}} at {{disk.usage_percent}}%"
      - type: suggest
        command: "doctor.clean_tmp"
        
  - id: memory-pressure-high
    name: "Memory Pressure High"
    severity: high
    condition: "memory.available_mb < 512 AND memory.pressure > 80"
    duration: "30s"
    auto_resolve: true
    actions:
      - type: alert
      - type: suggest
        command: "doctor.restart_services"
        
  - id: nxs-unresponsive
    name: "NXS Unresponsive"
    severity: critical
    condition: "nxs.last_heartbeat > 60s"
    duration: "0s"  # Immediate
    auto_resolve: false
    actions:
      - type: alert
      - type: escalate
        after: "5m"
        action: "doctor.restart_nxs"
```

**Built-in Rule Categories:**
| Category | Rules | Triggers |
|----------|-------|----------|
| Resources | 8 | CPU, memory, disk, inode, FD usage |
| Network | 5 | Connectivity, latency, DNS, Tailscale |
| Services | 4 | NXS, gateway, voice, browser health |
| Security | 3 | Failed auth, permission changes, file integrity |
| Performance | 4 | Response time, queue depth, error rates |

**Custom Rule Support:**
- User-defined rules in `~/.openclaw/doctor/rules/`
- Same YAML format as built-in
- Hot-reload on file change
- Validation on load (reject invalid rules)

**Benefit:** Specific, actionable diagnostics; extensible; clear severity mapping.

#### 5. **Alert Correlation & Root Cause Analysis** (New)
**Gap Found:** Individual alerts may have common root cause. Correlating reduces noise and speeds resolution.

**Refinement:** Implement temporal and topological correlation:
```
Correlation Dimensions:

1. Temporal Clustering
   - Alerts within 60 seconds of each other
   - Same severity or cascading (critical→high→medium)
   - Group as "incident" with primary and related alerts

2. Topological Correlation
   - Service dependency graph: A → B means A depends on B
   - If B fails, A failure is likely consequence, not separate cause
   - Root cause = deepest dependency in failure chain

3. Pattern Matching
   - Learned patterns from historical incidents
   - "Disk full + service crash" → disk is root cause
   - "Network latency + timeouts" → network is root cause
```

**Incident Grouping Example:**
```
[INCIDENT-2026-0222-001] Critical
├── Root Cause: disk-space-critical (/) at 98%
├── Related: service-restart-failed (NXS)
├── Related: high-error-rate (API)
└── Suggested Action: Free disk space, then restart NXS
```

**Root Cause Confidence Scoring:**
```
Confidence = base_score × temporal_factor × topological_factor

Base scores:
- Resource exhaustion: 0.9
- Network partition: 0.85
- Service crash: 0.7
- Configuration error: 0.6

Temporal factor: Decay with time since alert (1.0 at t=0, 0.5 at t=5min)
Topological factor: Depth in dependency graph (leaf=1.0, root=0.6)
```

**Benefit:** Fewer, more actionable alerts; faster root cause identification; reduced cognitive load.

#### 6. **Predictive Health Scoring** (New)
**Gap Found:** Reactive alerting only. No early warning based on trends.

**Refinement:** Implement predictive health metrics:
```
Health Dimensions (0-100 score):

1. Resource Trend Score
   - Linear regression on resource usage over 1 hour
   - Predict time-to-exhaustion
   - Score: 100 - (hours_until_exhaustion × 10)
   - If < 20: Predictive alert "Disk will be full in 2 hours"

2. Error Rate Trend
   - Exponential moving average of error rate
   - Compare to baseline (7-day average)
   - Score: 100 - ((current_rate - baseline) / baseline × 100)
   - If < 50: Alert "Error rate increasing, may indicate degradation"

3. Anomaly Detection
   - Statistical outliers in metrics (Z-score > 3)
   - Unusual patterns: CPU spikes, memory leaks, I/O storms
   - Score: 100 - (anomaly_severity × 20)

Overall Health Score: Weighted average of dimensions
```

**Predictive Alert Examples:**
- "Memory usage trending up. At current rate, will exhaust in 3 hours."
- "Error rate 3x baseline. Likely service degradation."
- "Unusual disk write pattern detected. Possible runaway log."

**Benefit:** Early warning before failure; time to intervene; trend visibility.

---

## Task R008: Tailscale Integration - Key Rotation & Audit Deep Analysis

### Current Status
R008 has HA with subnet routers, DERP routing, Headscale fallback, and key expiry handling. Previous refinements covered failover behavior and regional routing. Identified gaps in key rotation procedures, audit logging, and multi-tenant scenarios.

### Refinements Identified

#### 7. **Automated Key Rotation Protocol** (New)
**Gap Found:** Key expiry handling mentions proactive renewal, but no concrete rotation protocol for compromise scenarios.

**Refinement:** Design 3-tier key rotation strategy:
```
Rotation Triggers:

1. Scheduled Rotation (every 90 days)
   - Generate new key pair 7 days before expiry
   - Distribute to all instances via encrypted channel
   - Grace period: Both keys valid for 24 hours
   - Revoke old key after grace period

2. Compromise Response (immediate)
   - Emergency rotation on suspected compromise
   - Generate new key pair immediately
   - Revoke old key within 5 minutes
   - Brief interruption acceptable for security

3. Personnel Change (on-demand)
   - When human with key access departs
   - Rotate all keys they had access to
   - Audit trail of rotation reason
```

**Rotation Procedure:**
```bash
# Automated rotation script (doctor tailscale rotate-keys)
1. Generate new auth key via Tailscale API
2. Encrypt new key with instance public keys
3. Distribute to all instances via secure channel
4. Update Tailscale ACLs with new key
5. Verify all instances can authenticate
6. Revoke old key
7. Log rotation event with reason
```

**Zero-Downtime Rotation:**
- Pre-auth keys: Generate before needed, activate during rotation
- Node keys: Use Tailscale's built-in key renewal (transparent)
- API keys: Dual-key period with automatic failover

**Emergency Lockdown:**
- Compromise detected → Immediate ACL lock to isolate
- Emergency key for Architect access only
- Gradual restoration after investigation

**Benefit:** Proactive security; minimal interruption; clear compromise response.

#### 8. **Comprehensive Audit Logging** (New)
**Gap Found:** No mention of audit trails for Tailscale operations. Compliance and forensics require logging.

**Refinement:** Define audit event taxonomy:
```
Audit Events to Log:

Network Events:
- node.join: New node added to tailnet
- node.leave: Node removed from tailnet
- node.key_rotate: Key rotation occurred
- node.tags_change: ACL tags modified

Access Events:
- acl.allow: Connection allowed by ACL
- acl.deny: Connection denied by ACL
- subnet.route_add: Subnet route advertised
- subnet.route_remove: Subnet route withdrawn

Admin Events:
- admin.login: Admin access to Tailscale console
- admin.acl_change: ACL policy modified
- admin.device_approve: Device manually approved
- admin.device_remove: Device removed from tailnet

Security Events:
- security.key_compromise: Emergency rotation triggered
- security.anomaly: Unusual access pattern detected
- security.lockdown: Emergency ACL lock activated
```

**Log Format:**
```json
{
  "timestamp": "2026-02-22T12:00:00Z",
  "event_type": "node.join",
  "severity": "info",
  "actor": "nxs-instance-2",
  "target": "100.64.1.5",
  "details": {
    "os": "linux",
    "version": "1.56.0",
    "tags": ["tag:nxs", "tag:compute"]
  },
  "signature": "[Ed25519 signature]"
}
```

**Log Storage:**
- Local: 30 days retention on each instance
- Central: GitHub gist with append-only log
- Archive: Weekly export to cold storage
- Tamper protection: Signed log entries

**Audit Query Interface:**
```bash
# Query audit log
doctor audit query --since "24h" --type "security.*"
doctor audit report --weekly --format markdown
```

**Benefit:** Compliance ready; forensic capability; anomaly detection feed.

---

## Cross-Task Integration Insights

### R002 + R006: Health-Aware Load Balancing
The Doctor's health scores (R006) should feed into R002's load balancing:
- DEGRADED instances receive reduced traffic weight
- Predictive health alerts trigger proactive migration
- Incident correlation helps identify systemic issues

### R006 + R008: Network Health Monitoring
The Doctor should monitor Tailscale health (R008):
- Track DERP relay latency per instance
- Alert on subnet router failover events
- Monitor key expiry and rotation status
- Correlate network issues with service degradation

### R002 + R008: Mesh-Aware Instance Coordination
Instance discovery (R002) should leverage Tailscale (R008):
- Use Tailscale MagicDNS for automatic peer discovery
- Subnet router health affects instance routing decisions
- Network partitions detected via Tailscale connectivity

---

## Updated Task Status

| Task | New Improvements | Total Improvements |
|------|------------------|-------------------|
| R002 | 3 | 8 |
| R006 | 3 | 9 |
| R008 | 2 | 10 |
| **Total** | **8** | **58** |

---

## Files Updated

1. `/opt/development/memory/2026-02-22.md` — This research log (appended)
2. `/opt/development/RESEARCH-TASK-INDEX.md` — Added R002, R006, R008 refinements

---

## Backup Status

Research results will be backed up to GitHub via API after session completion.

---

*Session completed: 2026-02-22T20:05:00+08:00*

**Researcher:** Tenet Ashmier Manju (NXS Development Agent)  
**Hardware Usage:** 35-40% RAM, 26% disk (within limits)  
**Sub-agents Used:** 0/2

**Refinements Documented:** 8 improvements across 3 research tasks  
**Status:** Continuous improvement mode — all tasks remain Ongoing
