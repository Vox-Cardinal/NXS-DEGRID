# NXS Research Log

## Research Session: 2026-02-20 (Late Evening)

### Task R001: XTTS-v2 Integration - COMPLETED

**Timestamp Completed:** 2026-02-20T23:05:00+08:00

---

## R001 Final Findings: XTTS-v2 API Server Integration Plan

### Overview
XTTS-v2 is a voice cloning TTS model by Coqui AI (now discontinued but open source). Best integration path is via the `xtts-api-server` FastAPI wrapper.

### Server Options

| Option | Command | Best For |
|--------|---------|----------|
| **Docker** | `docker run -d daswer123/xtts-api-server` | Quick start, isolation |
| **pip install** | `pip install xtts-api-server` | Customization, development |
| **Manual** | Clone + venv + pip | Full control |

### Recommended Setup for NXS

**Option A: Docker (Production)**
```bash
# Run with persistent storage
docker run -d \
  --name xtts-server \
  -p 8020:8020 \
  -v ~/xtts/models:/app/models \
  -v ~/xtts/speakers:/app/speakers \
  -v ~/xtts/output:/app/output \
  --restart unless-stopped \
  daswer123/xtts-api-server
```

**Option B: Python venv (Development)**
```bash
sudo apt install -y python3-dev python3-venv portaudio19-dev
python3 -m venv ~/xtts-venv
source ~/xtts-venv/bin/activate
pip install xtts-api-server
pip install torch==2.1.1+cpu torchaudio==2.1.1+cpu --index-url https://download.pytorch.org/whl/cpu
python -m xtts_api_server --listen --device cpu
```

### Resource Requirements

| Resource | Minimum | Recommended | Notes |
|----------|---------|-------------|-------|
| **RAM** | 2GB | 4GB | Model loads into RAM |
| **Disk** | 1GB | 2GB | Model + samples + output |
| **CPU** | Any | 2+ cores | CPU mode works fine |
| **GPU** | Optional | CUDA 11.8+ | 3x speedup with GPU |

### Model Sizes

| Version | Size | Location |
|---------|------|----------|
| XTTS-v2.0.2 | ~400MB | Downloaded on first run |
| XTTS-v2.0.3 | ~400MB | Specify with `-v v2.0.3` |
| speakers_xtts.pth | 7.75MB | Pre-trained speaker embeddings |

### API Endpoints

Base URL: `http://localhost:8020`

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/docs` | GET | OpenAPI documentation |
| `/tts_to_audio` | POST | Generate audio from text |
| `/tts_to_file` | POST | Save audio to file |
| `/speakers` | GET | List available voice samples |
| `/speakers_list` | GET | Simple speaker list |
| `/set_tts_settings` | POST | Configure generation params |

### Key API Parameters

**POST /tts_to_audio**
```json
{
  "text": "Hello, this is a test.",
  "speaker_wav": "speakers/sample.wav",
  "language": "en"
}
```

**Supported Languages:**
- `en` - English
- `zh` - Chinese
- `ja` - Japanese
- `ko` - Korean
- `es` - Spanish
- `fr` - French
- `de` - German
- `it` - Italian
- `pt` - Portuguese
- `pl` - Polish
- `tr` - Turkish
- `ru` - Russian
- `nl` - Dutch
- `cs` - Czech
- `ar` - Arabic
- `hu` - Hungarian
- `hi` - Hindi

### Server Startup Flags

| Flag | Purpose |
|------|---------|
| `--listen` / `-hs 0.0.0.0` | Allow external connections |
| `-p 8020` | Custom port |
| `--device cpu` / `--device cuda` | CPU or GPU mode |
| `--lowvram` | Store model in RAM, move to VRAM for processing |
| `--deepspeed` | 2-3x speedup (auto-downloads libs) |
| `--use-cache` | Cache results for repeated requests |
| `--streaming-mode` | Stream audio chunks (local only) |
| `-sf /path/to/speakers` | Custom speaker samples folder |
| `-o /path/to/output` | Custom output folder |

### Voice Cloning Setup

1. Create `speakers/` folder
2. Add WAV files (7-9 seconds, mono, 22050Hz, 16-bit)
3. Reference by filename in API calls
4. Multiple samples per speaker = better quality

### NXS Integration Architecture

```
[NXS Agent] → HTTP POST → [XTTS API Server:8020] → WAV File
                ↑
[Whisper STT] ← Voice Input
```

**Integration Pattern:**
1. NXS receives text to speak
2. Calls XTTS API with text + speaker reference
3. Receives audio file path or binary
4. Plays audio or sends to user

### Security Considerations

- Use `--listen` only with firewall/Tailscale
- Default localhost binding is safe
- No authentication built-in (use reverse proxy if needed)
- Model download requires HuggingFace access (first run)

### Comparison with Alternatives

| Feature | XTTS-v2 | Piper | Coqui TTS |
|---------|---------|-------|-----------|
| Voice Cloning | ✅ Yes | ❌ No | ⚠️ Limited |
| Quality | ⭐⭐⭐ Excellent | ⭐⭐ Good | ⭐⭐⭐ Excellent |
| Speed | Medium | Fast | Medium |
| Size | ~1GB | ~100MB | ~500MB |
| License | MPL | MIT | MPL |

### Recommendation for NXS

**Primary:** XTTS-v2 with Docker deployment
- Best voice quality
- Voice cloning capability
- Easy containerization
- Fits within 75% resource constraint (4GB RAM system)

**Fallback:** Piper for lightweight scenarios
- If XTTS too heavy
- Faster response
- Lower quality acceptable

---

## Task R002: Whisper STT Integration - IN PROGRESS

**Timestamp Started:** 2026-02-20T23:27:00+08:00

---

## R002 Findings: Whisper STT Integration Research

### Overview
Speech-to-Text (STT) is the counterpart to XTTS-v2 for voice interaction. Multiple Whisper implementations exist, each optimized for different use cases.

### Key Whisper Variants Comparison

| Variant | Backend | Best For | Speed | Resource Usage |
|---------|---------|----------|-------|----------------|
| **faster-whisper** | CTranslate2 | Production, CPU/GPU | Fast | Moderate |
| **whisper.cpp** | ggml | Edge devices, C++ | Medium | Low |
| **insanely-fast-whisper** | HF Transformers | Batch GPU processing | Very Fast | High GPU |
| **WhisperX** | faster-whisper + extras | Diarization, timestamps | Medium | Higher |

### faster-whisper vs whisper.cpp

Based on community benchmarks (GitHub issue #1127):
- **faster-whisper**: 14s for small.en model on CPU
- **whisper.cpp**: 46s for equivalent model on CPU
- **Winner**: faster-whisper for CPU deployments

### Recommended for NXS: faster-whisper

**Rationale:**
1. **Fastest on CPU** - Critical for resource-constrained environments
2. **Quantization support** - INT8/FP16 reduces memory
3. **Production-ready** - Most widely deployed
4. **Easy Docker deployment** - Matches XTTS-v2 pattern

### Server Options

| Server | Features | Best For |
|--------|----------|----------|
| **speaches** (formerly faster-whisper-server) | OpenAI API compatible, TTS+STT, streaming | Full-featured deployment |
| **faster-whisper-server** (fedirz) | OpenAI API compatible, lightweight | Simple STT-only |
| **Fast-Powerful-Whisper-AI** | Async API, multi-GPU, distributed | Enterprise scale |

### Recommended: speaches (formerly faster-whisper-server)

**Why speaches:**
- OpenAI API compatible (drop-in replacement)
- Supports both STT (faster-whisper) and TTS (Kokoro, Piper)
- Dynamic model loading/offloading
- Streaming transcription via SSE
- Realtime API support
- GPU and CPU support
- Docker deployment

### Docker Deployment

```bash
# CPU-only deployment
docker run -d \
  --name speaches \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  --restart unless-stopped \
  ghcr.io/speaches-ai/speaches:latest-cpu

# With GPU support
docker run -d \
  --name speaches \
  --gpus all \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  --restart unless-stopped \
  ghcr.io/speaches-ai/speaches:latest-cuda
```

### Model Size Comparison

| Model | Parameters | Disk Size | VRAM/RAM | Speed | Accuracy |
|-------|------------|-----------|----------|-------|----------|
| **tiny** | 39M | ~75MB | ~1GB | Fastest | Basic |
| **base** | 74M | ~150MB | ~1GB | Fast | Good |
| **small** | 244M | ~500MB | ~2GB | Medium | Better |
| **medium** | 769M | ~1.5GB | ~5GB | Slow | Excellent |
| **large** | 1550M | ~3GB | ~10GB | Slowest | Best |
| **turbo** | 809M | ~1.6GB | ~6GB | Fast | Excellent |

### NXS Recommendation

**For 4GB RAM system:**
- **Primary**: `base` or `small` model
- **Fallback**: `tiny` for real-time scenarios
- **Avoid**: medium/large (exceed resource constraints)

### API Endpoints (OpenAI Compatible)

Base URL: `http://localhost:8000`

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/v1/audio/transcriptions` | POST | Transcribe audio to text |
| `/v1/audio/translations` | POST | Translate audio to English |
| `/v1/audio/speech` | POST | Text-to-speech (Kokoro/Piper) |
| `/v1/models` | GET | List available models |
| `/health` | GET | Health check |

### Example API Call

```bash
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -H "Content-Type: multipart/form-data" \
  -F file=@audio.wav \
  -F model=base \
  -F language=en
```

### Integration Architecture

```
[User Voice] → [Whisper API:8000] → [Text] → [NXS Agent]
                                              ↓
[User Audio] ← [XTTS API:8020] ← [Response] ←┘
```

### Resource Requirements (faster-whisper base model)

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| **RAM** | 1GB | 2GB |
| **Disk** | 500MB | 1GB |
| **CPU** | 1 core | 2+ cores |
| **GPU** | Optional | CUDA 11.8+ |

### Comparison: XTTS-v2 vs speaches (TTS)

| Feature | XTTS-v2 | speaches (Kokoro) |
|---------|---------|-------------------|
| Voice Cloning | ✅ Yes | ❌ No |
| Quality | ⭐⭐⭐ Excellent | ⭐⭐⭐ Excellent |
| Speed | Medium | Fast |
| Size | ~1GB | ~300MB |
| Setup | Separate container | Built-in |

### NXS Voice Stack Recommendation

**Option A: Separate Services (Current Plan)**
- XTTS-v2 on port 8020 (TTS with voice cloning)
- speaches on port 8000 (STT only, base model)
- Pros: Best TTS quality, voice cloning
- Cons: Two containers, more resource usage

**Option B: Unified Service (Alternative)**
- speaches only on port 8000 (STT + TTS)
- Use Kokoro for TTS (no voice cloning)
- Pros: Single container, simpler
- Cons: No voice cloning

### Next Steps for R002

1. Test speaches Docker deployment
2. Compare base vs small model performance
3. Test streaming transcription
4. Document integration with NXS skill

---

## Research Status Update

| ID | Task | Status | Notes |
|----|------|--------|-------|
| R000 | NXS Codebase Analysis | **Complete** | Size reduction plan documented |
| R001 | XTTS-v2 Integration | **Complete** | Docker + pip options documented |
| R002 | Whisper STT Integration | **In Progress** | faster-whisper + speaches identified |
| R003 | The Doctor Architecture | Planned | Can use remote monitoring |
| R004 | URL Frontend Design | Planned | See R007 for bridge protocol |
| R005 | Tailscale Integration | Planned | Enables R008 |
| R006 | ComfyUI API Pattern | Planned | External API example |
| R007 | Kimi-Claw Plugin Analysis | Planned | Bridge protocol for R004 |
| R008 | Cross-Machine Agent Communication | In Progress | Architecture defined |
| R009 | LM Studio Provider Research | Planned | Context window handling |

---

## Next Research Priority

**R003: The Doctor Architecture** - Self-monitoring daemon design. Research:
- System health monitoring patterns
- Permission-based action framework
- Integration with NXS agent decision loop
- Resource cleanup strategies

*Research scheduled for next cron run.*

---

*Session ended: 2026-02-20T23:45:00+08:00*
