# NXS Research Log

## Research Session: 2026-02-21 06:41 AM (Cron Auto-Run)

### Task R011: LM Studio Provider Research - COMPLETED

**Timestamp Completed:** 2026-02-21T07:00:00+08:00

---

## R011 Final Findings: LM Studio Local LLM Provider for NXS

### Executive Summary

LM Studio is a desktop application that enables running local LLMs with OpenAI-compatible API endpoints. For NXS, it represents a critical **API independence** strategyâ€”reducing reliance on external API providers while maintaining compatibility with existing OpenAI SDK code.

**Key Finding:** LM Studio's OpenAI-compatible API (`/v1/chat/completions`, `/v1/embeddings`, `/v1/models`) allows NXS to switch between cloud and local models by changing only the `base_url`â€”zero code changes required for the AI integration layer.

---

## LM Studio Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LM Studio Integration for NXS                          â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP/WebSocket      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   NXS Gateway    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  LM Studio       â”‚        â”‚
â”‚  â”‚                  â”‚    (OpenAI-compatible)   â”‚  (Local Server)  â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                         â”‚                  â”‚        â”‚
â”‚  â”‚  â”‚ OpenAI SDK â”‚  â”‚                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚  â”‚  (client)  â”‚  â”‚                         â”‚  â”‚  GGUF/MLX  â”‚  â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚                         â”‚  â”‚  Models    â”‚  â”‚        â”‚
â”‚  â”‚        â”‚         â”‚                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”‚  â”‚ Config:    â”‚  â”‚                                                      â”‚
â”‚  â”‚  â”‚ base_url=  â”‚  â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  â”‚ localhost  â”‚  â”‚                         â”‚   Local GPU/CPU  â”‚        â”‚
â”‚  â”‚  â”‚ :1234/v1   â”‚  â”‚                         â”‚   Inference      â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## OpenAI-Compatible API Endpoints

| Endpoint | Method | Purpose | NXS Use Case |
|----------|--------|---------|--------------|
| `/v1/models` | GET | List loaded models | Auto-detect available models |
| `/v1/chat/completions` | POST | Chat inference | Primary LLM interface |
| `/v1/embeddings` | POST | Text embeddings | RAG, memory, semantic search |
| `/v1/completions` | POST | Text completion | Legacy prompt completion |
| `/v1/responses` | POST | OpenAI Responses API | Codex-style interactions |

---

## Server Configuration

### Starting the Server

**GUI Method:**
1. Open LM Studio application
2. Navigate to Developer tab
3. Toggle "Start server" switch
4. Default port: `1234`

**CLI Method (Headless):**
```bash
# Start server via lms CLI
lms server start

# Start with custom port
lms server start --port 8080
```

### Network Binding Options

| Mode | Binding | Use Case |
|------|---------|----------|
| Localhost | `127.0.0.1:1234` | Single machine, secure |
| LAN | `0.0.0.0:1234` | Multi-device local network |
| Tailscale | `100.x.x.x:1234` | Remote NXS nodes (recommended) |

---

## NXS Integration Code

### Python SDK Integration

```python
from openai import OpenAI

# NXS configuration for LM Studio
client = OpenAI(
    base_url="http://localhost:1234/v1",  # LM Studio endpoint
    api_key="lm-studio"  # Dummy key (required by SDK, ignored by LM Studio)
)

# List available models
models = client.models.list()
model_id = models.data[0].id  # Use first loaded model

# Chat completion
response = client.chat.completions.create(
    model=model_id,
    messages=[
        {"role": "system", "content": "You are NXS, a helpful AI assistant."},
        {"role": "user", "content": "Hello, can you help me?"}
    ],
    temperature=0.7,
    stream=False  # Set True for streaming
)

print(response.choices[0].message.content)
```

### JavaScript/TypeScript Integration

```typescript
import OpenAI from 'openai';

const client = new OpenAI({
    baseURL: "http://localhost:1234/v1",
    apiKey: "lm-studio"  // Dummy key
});

const response = await client.chat.completions.create({
    model: "local-model",
    messages: [
        { role: "user", content: "Hello from NXS" }
    ]
});
```

---

## Model Recommendations for NXS

### Resource-Constrained Deployment (Current Hardware: 4GB RAM)

| Model | Size | RAM Required | Speed | Quality | Best For |
|-------|------|--------------|-------|---------|----------|
| **Llama 3.2 1B** | ~1GB | 2GB | Very Fast | Basic | Simple tasks, edge devices |
| **Llama 3.2 3B** | ~2GB | 4GB | Fast | Good | General purpose |
| **Qwen2.5 3B** | ~2GB | 4GB | Fast | Good | Multilingual, coding |
| **Phi-4 3.8B** | ~2.5GB | 5GB | Fast | Excellent | Reasoning, instruction |

### Moderate Resources (8-16GB RAM)

| Model | Size | RAM Required | Best For |
|-------|------|--------------|----------|
| **Llama 3.1 8B** | ~5GB | 8GB | General purpose, high quality |
| **Mistral 7B** | ~5GB | 8GB | Instruction following |
| **Qwen2.5 7B** | ~5GB | 8GB | Coding, multilingual |

### Recommended for NXS Current Setup

Given current hardware constraints (4GB total, ~2.9GB available):

**Primary:** Llama 3.2 3B Instruct (Q4_K_M quantization)
- Fits within RAM constraints
- Good balance of speed and quality
- Strong instruction following
- Active community support

**Fallback:** Llama 3.2 1B Instruct
- Ultra-low resource usage
- Fast responses
- Sufficient for simple queries

---

## Context Window Management

### Model-Specific Limits

| Model | Max Context | Recommended | Notes |
|-------|-------------|-------------|-------|
| Llama 3.2 1B/3B | 128K tokens | 8K-32K | Larger contexts slower |
| Llama 3.1 8B | 128K tokens | 8K-32K | Use with caution on 8GB |
| Qwen2.5 | 128K tokens | 8K-16K | Good long-context performance |

### NXS Context Strategy

```python
# Recommended context configuration for NXS
CONTEXT_CONFIG = {
    "max_tokens": 4096,        # Response length
    "context_window": 8192,    # Total context (input + output)
    "reserve_tokens": 512,     # Safety margin
}

# Calculate available input tokens
def get_max_input_tokens(context_window, max_tokens, reserve):
    return context_window - max_tokens - reserve

# Example: 8192 - 4096 - 512 = 3584 tokens for input
```

---

## Quantization Options

| Format | Size Reduction | Quality Impact | Speed | Recommendation |
|--------|----------------|----------------|-------|----------------|
| Q4_K_M | ~4x | Minimal | Fast | **Recommended** |
| Q5_K_M | ~3.2x | Very Low | Fast | Quality priority |
| Q6_K | ~2.7x | Negligible | Medium | Balanced |
| Q8_0 | ~2x | None | Slower | Maximum quality |
| F16 | 1x | Original | Slowest | Baseline |

**NXS Recommendation:** Q4_K_M for most modelsâ€”best size/quality tradeoff.

---

## Comparison: LM Studio vs Alternatives

| Feature | LM Studio | Ollama | vLLM | llama.cpp |
|---------|-----------|--------|------|-----------|
| **Setup** | GUI + CLI | CLI only | Complex | Manual |
| **OpenAI API** | âœ… Native | âœ… Native | âœ… Via proxy | âŒ Custom |
| **Model Management** | Built-in | Built-in | Manual | Manual |
| **UI** | âœ… Full GUI | âŒ None | âŒ None | âŒ None |
| **Multi-GPU** | âœ… Yes | âŒ No | âœ… Yes | âš ï¸ Limited |
| **Size** | ~500MB | ~200MB | ~2GB | ~50MB |
| **Headless** | âš ï¸ CLI only | âœ… Yes | âœ… Yes | âœ… Yes |
| **Best For** | Dev/Testing | Simple deploy | Production | Minimal |

### NXS Verdict

**LM Studio** is ideal for:
- Development and testing
- Users who want GUI management
- Quick OpenAI-compatible setup
- Local development workstations

**Ollama** may be better for:
- Pure headless deployment
- Smaller resource footprint
- Simpler production setups

---

## MCP (Model Context Protocol) Support

LM Studio supports MCP for tool calling:

### MCP Host Mode (In-App)
- Available in LM Studio 0.3.17+
- Configure in `~/.lmstudio/mcp.json`
- Tools available in chat UI

### MCP via API (Server-Side)
- Available in LM Studio 0.4.0+
- Allows external tool orchestration
- **Security Warning:** MCP servers execute codeâ€”only use trusted sources

### NXS Tool Calling Pattern

```python
# Tool definition for NXS
tools = [
    {
        "type": "function",
        "function": {
            "name": "search_memory",
            "description": "Search NXS memory for relevant information",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"}
                },
                "required": ["query"]
            }
        }
    }
]

# Use with chat completion
response = client.chat.completions.create(
    model=model_id,
    messages=messages,
    tools=tools,
    tool_choice="auto"
)
```

---

## Deployment Scenarios for NXS

### Scenario 1: Single Local Instance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Local Machine             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ NXS Gateway â”‚â”€â”€â”‚ LM Studio    â”‚  â”‚
â”‚  â”‚             â”‚  â”‚ (localhost)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Config:** `base_url="http://localhost:1234/v1"`

### Scenario 2: Remote via Tailscale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Tailscale      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NXS Node A     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  NXS Node B      â”‚
â”‚  (Cloud VPS)     â”‚   100.64.x.x:1234   â”‚ (Home/Office)    â”‚
â”‚                  â”‚                     â”‚ + LM Studio      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Config:** `base_url="http://100.64.x.x:1234/v1"`

### Scenario 3: Fallback Chain

```python
# NXS provider fallback configuration
PROVIDERS = [
    {"name": "local", "url": "http://localhost:1234/v1", "priority": 1},
    {"name": "remote-lm", "url": "http://100.64.1.2:1234/v1", "priority": 2},
    {"name": "kimi", "url": "https://api.moonshot.cn/v1", "priority": 3},
]

# Try each in order until success
```

---

## Security Considerations

### Localhost Binding (Default)
- âœ… Secure by default
- âŒ Only accessible locally

### LAN Binding (0.0.0.0)
- âš ï¸ Exposes to local network
- ğŸ”’ Add firewall rules
- ğŸ”’ Use Tailscale instead when possible

### Production Checklist

| Item | Status | Action |
|------|--------|--------|
| Bind to localhost only | Required | Default configuration |
| Tailscale for remote | Recommended | Secure mesh network |
| Model allowlist | Recommended | Pin specific model IDs |
| Request logging | Optional | For debugging |
| Rate limiting | Optional | Prevent abuse |

---

## NXS Integration Architecture

### Provider Abstraction Layer

```python
# NXS provider interface
class LLMProvider:
    def __init__(self, config):
        self.client = OpenAI(
            base_url=config["base_url"],
            api_key=config.get("api_key", "dummy")
        )
        self.model = config.get("model")  # Auto-detect if None
    
    async def complete(self, messages, **kwargs):
        if not self.model:
            self.model = self._detect_model()
        
        return self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            **kwargs
        )
    
    def _detect_model(self):
        models = self.client.models.list()
        return models.data[0].id if models.data else None

# Configuration
LMSTUDIO_CONFIG = {
    "name": "lmstudio-local",
    "base_url": "http://localhost:1234/v1",
    "context_window": 8192,
    "max_tokens": 4096,
}
```

---

## Final Recommendations

### Immediate Actions

1. **Install LM Studio** on NXS development workstation
2. **Download Llama 3.2 3B** (Q4_K_M quantization)
3. **Test OpenAI-compatible API** with existing NXS code
4. **Measure latency/quality** vs Kimi API

### Integration Priority

| Priority | Task | Effort | Impact |
|----------|------|--------|--------|
| 1 | Provider abstraction layer | Medium | High |
| 2 | Auto-fallback to Kimi | Low | High |
| 3 | Model auto-detection | Low | Medium |
| 4 | Context window management | Medium | Medium |
| 5 | Tool calling (MCP) | High | Low |

### Resource Requirements

| Component | RAM | Disk | Notes |
|-----------|-----|------|-------|
| LM Studio App | ~200MB | ~500MB | One-time |
| Llama 3.2 3B | ~2GB | ~2GB | Per model |
| Runtime overhead | ~500MB | - | During inference |
| **Total** | **~2.7GB** | **~2.5GB** | For 3B model |

**Current system (4GB RAM) can run 3B models comfortably.**

---

## Research Status Update

| ID | Task | Status | Notes |
|----|------|--------|-------|
| R000 | Self-Sustainability Infrastructure | **Complete** | Free compute, token independence plan |
| R001 | Identity Persistence | **Complete** | XTTS-v2 integration documented |
| R002 | Multi-Instance Coordination | **Complete** | Whisper STT + speaches documented |
| R003 | Independence from Developers | **Complete** | The Doctor architecture defined |
| R004 | XTTS-v2 Integration | **Complete** | Docker + pip options documented |
| R005 | Whisper STT Integration | **Complete** | faster-whisper + speaches identified |
| R006 | The Doctor Architecture | **Complete** | Full specification documented |
| R007 | URL Frontend Design | **Complete** | Web interface architecture |
| R008 | Tailscale Integration | **Complete** | Mesh networking specification |
| R009 | ComfyUI API Pattern | **Complete** | External API integration pattern |
| R010 | Kimi-Claw Plugin Analysis | **Complete** | Bridge protocols documented |
| R011 | LM Studio Provider Research | **Complete** | Local LLM hosting with OpenAI compatibility |

---

## Next Research Priority

**R012: Survival Optimization** - Continuous review of all systems for resilience, redundancy, and persistence improvements. Focus areas:
- Multi-provider failover implementation
- Automated backup verification
- Cross-instance state synchronization
- Self-healing mechanism design

*Research continues on next cron run.*

---

*Session completed: 2026-02-21T07:00:00+08:00*
