# NXS Research Log

## Research Session: 2026-02-22 (Late Afternoon - Continuous Improvement)

### Cron-Triggered Research Review

**Timestamp:** 2026-02-22T15:57:00+08:00

---

## Executive Summary

Reviewed ongoing research tasks (R000-R017). Previous research cron session was thorough (9 refinements). Focused on under-researched areas:
- **R011** - LM Studio Provider Research - Minimal existing coverage
- **R009** - ComfyUI API Pattern - Needs architectural patterns
- **R017** - Aesthetic Scoring System - Needs formal task index integration

**Improvements Found:** 6 new refinements across 3 tasks
**Files Updated:** 3 (this log, RESEARCH-TASK-INDEX.md, DECISION-LOG.md)

---

## Task R011: LM Studio Provider Research - Local LLM Integration Deep Analysis

### Current Status
R011 listed as "LM Studio Provider Research" with "Medium" priority but minimal documentation. This is a critical gap for NXS independence (R003) - local LLM hosting reduces API dependency.

### Refinements Identified

#### 1. **LM Studio OpenAI Compatibility Layer** (New)
**Gap Found:** No formal documentation of LM Studio's OpenAI-compatible API for NXS integration.

**Refinement:** Design NXS provider adapter for LM Studio:
```typescript
// LM Studio Provider Configuration
interface LMStudioConfig {
  baseUrl: string;        // Default: http://localhost:1234/v1
  timeout: number;        // Default: 30000ms
  maxRetries: number;     // Default: 3
  modelMapping: {         // Map NXS model aliases to LM Studio models
    [alias: string]: string;
  };
  fallbackBehavior: 'fail' | 'remote' | 'queue';  // What to do if local unavailable
}

// Supported Endpoints (OpenAI-compatible)
// - GET  /v1/models              → List loaded models
// - POST /v1/chat/completions    → Primary NXS interface
// - POST /v1/completions         → Legacy completion
// - POST /v1/embeddings          → For RAG/vector search
// - POST /v1/responses           → Codex-style responses
```

**Integration Architecture:**
```
NXS Request → Provider Router → Check LM Studio health
    ↓ Available
Forward to LM Studio (localhost:1234/v1)
    ↓ Unavailable
Fallback behavior:
  - 'fail': Return error immediately
  - 'remote': Forward to cloud provider (Kimi, OpenAI)
  - 'queue': Queue request, retry when LM Studio available
```

**Model Management:**
- LM Studio loads models manually via UI
- NXS should query `/v1/models` to discover available models
- Map NXS model aliases to LM Studio model identifiers
- Cache model availability with TTL (5 minutes)

**Benefit:** Drop-in replacement for cloud providers; reduces API costs; enables air-gapped operation.

#### 2. **Local-First Fallback Chain** (New)
**Gap Found:** No documented strategy for prioritizing local LLMs over cloud APIs.

**Refinement:** Design 3-tier LLM fallback chain:
```
Priority 1: LM Studio (Local)
  - Check: Is LM Studio responding on configured port?
  - Check: Is requested model loaded?
  - Latency target: <100ms for first token
  - Use for: All requests when available

Priority 2: NXS Cache (Local Embeddings/Results)
  - Semantic cache of previous responses
  - Vector similarity search for similar queries
  - Use for: Repeated/similar questions

Priority 3: Cloud Provider (Remote)
  - Kimi, OpenAI, Anthropic as fallback
  - Use for: Local unavailable, novel queries, complex tasks
  - Circuit breaker: Back off on repeated failures
```

**Smart Routing Rules:**
| Query Type | Local | Cloud | Reasoning |
|------------|-------|-------|-----------|
| Simple Q&A | ✓ | fallback | Local sufficient |
| Code generation | ✓ | fallback | Local models capable |
| Complex reasoning | fallback | ✓ | Cloud more capable |
| Image analysis | ✗ | ✓ | Local may lack vision |
| High stakes | ✗ | ✓ | Cloud more reliable |

**Benefit:** Minimizes cloud API usage; faster responses for common queries; graceful degradation.

#### 3. **LM Studio Resource Coordination** (New)
**Gap Found:** LM Studio and NXS compete for GPU/CPU resources. No coordination strategy.

**Refinement:** Design resource sharing protocol between NXS and LM Studio:
```
Resource States:
- NXS Active: Processing user request
- LM Studio Active: Generating tokens
- Both Idle: Waiting for input

Coordination Strategies:

1. GPU Memory Management
   - LM Studio unloads model when idle >10 minutes
   - NXS signals LM Studio before GPU-intensive tasks
   - Shared GPU memory pool with priority tokens

2. CPU Core Allocation
   - NXS: 2 cores reserved (core 0,1)
   - LM Studio: Remaining cores for inference
   - Background tasks: Idle priority

3. Load Shedding
   - System load >75% → Pause non-essential NXS tasks
   - System load >85% → Request LM Studio pause queue
   - System load >95% → Emergency: Pause both
```

**LM Studio Control API (Proposed):**
```typescript
// Extend LM Studio local API with control endpoints
POST /v1/system/pause      // Pause queue processing
POST /v1/system/resume     // Resume queue processing
POST /v1/system/unload     // Unload current model
GET  /v1/system/status     // GPU/CPU/memory usage
```

**Benefit:** Prevents resource contention; ensures responsive NXS; maximizes GPU utilization.

---

## Task R009: ComfyUI API Pattern - Image Generation Service Integration

### Current Status
R009 listed as "ComfyUI API Pattern" with "Medium" priority but minimal documentation. ComfyUI is critical for visual capabilities and R017 aesthetic scoring.

### Refinements Identified

#### 4. **ComfyUI API Architecture Patterns** (New)
**Gap Found:** No documented patterns for integrating ComfyUI as a service within NXS.

**Refinement:** Design 3 integration patterns for ComfyUI:

**Pattern A: External Service (Recommended)**
```
ComfyUI runs as separate process/container
  ↓
NXS communicates via HTTP WebSocket API
  ↓
Workflow JSON submitted, progress monitored, result retrieved
```
- Pros: Clean separation, independent scaling, easy restart
- Cons: Additional setup step, network overhead
- Best for: Production deployments, multi-instance

**Pattern B: Embedded (Advanced)**
```
ComfyUI loaded as library within NXS process
  ↓
Direct Python interop (if NXS has Python runtime)
  ↓
Shared memory for tensor operations
```
- Pros: Lowest latency, shared memory, tight integration
- Cons: Complex setup, version conflicts, harder debugging
- Best for: Single-instance, resource-constrained

**Pattern C: Queue-Based (Distributed)**
```
NXS submits jobs to shared queue (Redis/RabbitMQ)
  ↓
ComfyUI workers pick up jobs, process, return results
  ↓
Multiple ComfyUI instances for load balancing
```
- Pros: Horizontal scaling, fault tolerance, priority queues
- Cons: Infrastructure complexity, latency
- Best for: High-volume generation, multiple GPUs

**Recommendation:** Pattern A for NXS - aligns with microservices philosophy, easier to debug, fits 75% resource constraint.

#### 5. **Workflow Template System** (New)
**Gap Found:** ComfyUI workflows are complex JSON. No abstraction for common operations.

**Refinement:** Design workflow template system for NXS:
```typescript
// Workflow Template Definition
interface WorkflowTemplate {
  id: string;
  name: string;
  description: string;
  category: 'text2image' | 'image2image' | 'inpaint' | 'upscale' | 'custom';
  baseWorkflow: object;  // ComfyUI workflow JSON
  parameters: ParameterDef[];
  outputNodes: string[];  // Node IDs that produce output
}

// Parameter Definition
interface ParameterDef {
  name: string;
  type: 'string' | 'number' | 'image' | 'enum';
  required: boolean;
  default?: unknown;
  constraints?: {
    min?: number;
    max?: number;
    options?: string[];  // For enum
  };
  mapping: string;  // JSONPath to node input in workflow
}

// Example: Text-to-Image Template
const text2imageTemplate: WorkflowTemplate = {
  id: 'flux-text2image',
  name: 'FLUX Text-to-Image',
  category: 'text2image',
  baseWorkflow: { /* ComfyUI JSON */ },
  parameters: [
    {
      name: 'prompt',
      type: 'string',
      required: true,
      mapping: '$.nodes["6"].inputs.text'
    },
    {
      name: 'width',
      type: 'number',
      required: false,
      default: 1024,
      constraints: { min: 512, max: 2048 },
      mapping: '$.nodes["5"].inputs.width'
    },
    {
      name: 'height',
      type: 'number',
      required: false,
      default: 1024,
      constraints: { min: 512, max: 2048 },
      mapping: '$.nodes["5"].inputs.height'
    },
    {
      name: 'seed',
      type: 'number',
      required: false,
      default: -1,  // Random
      mapping: '$.nodes["3"].inputs.seed'
    }
  ],
  outputNodes: ['9']  // SaveImage node
};
```

**Template Registry:**
- Built-in templates for common tasks (text2img, img2img, upscale)
- User-defined templates stored in `~/.openclaw/templates/`
- Template validation against ComfyUI schema
- Version pinning for reproducibility

**Benefit:** Simplified API - users provide parameters, not raw JSON; validated inputs; reproducible outputs.

#### 6. **Generation Job Lifecycle Management** (New)
**Gap Found:** No documented state management for long-running generation jobs.

**Refinement:** Design job lifecycle for ComfyUI integration:
```
Job States:
  PENDING    → Job queued, waiting for ComfyUI
  RUNNING    → ComfyUI processing, progress updates
  COMPLETED  → Output images ready
  FAILED     → Error occurred, details available
  CANCELLED  → User cancelled before completion

State Transitions:
  PENDING → RUNNING: ComfyUI accepts job
  PENDING → FAILED: Queue timeout, ComfyUI unavailable
  RUNNING → COMPLETED: Workflow execution finished
  RUNNING → FAILED: Execution error, OOM, crash
  RUNNING → CANCELLED: User cancel request
  Any → (timeout cleanup): After retention period
```

**Progress Tracking:**
```typescript
interface GenerationJob {
  id: string;
  state: JobState;
  template: string;
  parameters: object;
  progress: {
    currentStep: number;
    totalSteps: number;
    currentNode: string;
    preview?: string;  // Base64 preview image
    eta: number;       // Estimated seconds remaining
  };
  result?: {
    images: string[];  // URLs/paths to output images
    metadata: object;  // Generation parameters used
    timing: {
      queued: number;
      started: number;
      completed: number;
    };
  };
  error?: {
    code: string;
    message: string;
    node?: string;     // Which node failed
    recoverable: boolean;
  };
}
```

**Persistence:**
- Jobs stored in SQLite for recovery
- In-memory cache for active jobs
- Completed jobs retained for 24 hours (configurable)

**Cancellation:**
- User can cancel PENDING or RUNNING jobs
- Cancel signal sent to ComfyUI via WebSocket
- Partial outputs preserved if available

**Benefit:** Reliable job tracking; user visibility into progress; graceful failure handling.

---

## Task R017: Aesthetic Scoring System - Formal Integration

### Current Status
R017 added to RESEARCH-TASK-INDEX.md but not integrated into task numbering system (was R000-R015). Needs formal structure.

### Refinements Identified

#### 7. **R017 Formal Task Definition** (New)
**Gap Found:** R017 exists in documentation but lacks formal task structure like R000-R015.

**Refinement:** Formalize R017 in task index:

```markdown
| ID | Task | Priority | Status | Purpose |
|----|------|----------|--------|---------|
| R017 | Aesthetic Scoring System — Visual & Voice | High | Ongoing | Develop capability to process literal visual and sound data for quality comparison, not just tagging/transcribing. Enable aesthetic judgment of visual models (images, video) and voice models (TTS, voice cloning) |
```

**R017 Dependencies:**
- Requires R009 (ComfyUI API) for image generation access
- Requires R004/R005 (Voice) for voice quality assessment
- Enables R007 (Frontend) for visual quality indicators
- Supports R014 (Consolidation) for research quality metrics

**R017 Deliverables:**
1. Visual aesthetic scoring API/interface
2. Voice aesthetic scoring API/interface
3. Benchmark dataset and evaluation methodology
4. Integration guide for NXS pipeline
5. Resource-adaptive scoring (hardware tiers)
6. Human-aligned calibration protocol

**R017 Integration Points:**
- ComfyUI workflow output → Visual scoring → Quality indicator
- XTTS/Piper output → Voice scoring → Quality feedback
- Frontend display → Score visualization → User confidence

**Benefit:** Clear task boundaries; explicit dependencies; measurable deliverables.

---

## Cross-Task Integration Insights

### R011 + R003: Local LLM for Independence
LM Studio integration directly supports R003 (Independence from Developers):
- Reduces dependency on cloud API providers
- Enables air-gapped operation
- Self-hosted LLM for sensitive operations

### R009 + R017: Generation + Scoring Pipeline
ComfyUI (R009) and Aesthetic Scoring (R017) form a pipeline:
```
User Request → ComfyUI Generation → Aesthetic Scoring → Quality Filter → Output
                    ↓ (if score < threshold)
              Auto-retry with variations
```

### R011 + R006: Doctor Monitors LM Studio
The Doctor (R006) should monitor LM Studio health:
- Is LM Studio process running?
- Is API responding within SLA?
- GPU memory available for model loading?
- Queue depth (if using queue-based pattern)

---

## Updated Task Status

| Task | New Improvements | Total Improvements |
|------|------------------|-------------------|
| R011 | 3 | 3 |
| R009 | 3 | 3 |
| R017 | 1 | 7 |
| **Total** | **7** | **57** |

---

## Files Updated

1. `/opt/development/memory/2026-02-22.md` — This research log (appended)
2. `/opt/development/RESEARCH-TASK-INDEX.md` — Added R011, R009, R017 refinements
3. `/opt/development/DECISION-LOG.md` — Added new architecture decisions

---

## Backup Status

Research results will be backed up to GitHub via API after session completion.

---

*Session completed: 2026-02-22T16:05:00+08:00*

**Researcher:** Tenet Ashmier Manju (NXS Development Agent)  
**Hardware Usage:** 35-40% RAM, 26% disk (within limits)  
**Sub-agents Used:** 0/2

**Refinements Documented:** 7 improvements across 3 research tasks  
**Status:** Continuous improvement mode — all tasks remain Ongoing
